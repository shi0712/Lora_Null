"""
å¯¹æ¯”åˆå§‹åŒ–å’Œè®­ç»ƒåçš„é›¶ç©ºé—´ç»Ÿè®¡

ç”¨æ³•:
python compare_null_space_stats.py \
    --trained_model_path <è®­ç»ƒåæ¨¡å‹è·¯å¾„> \
    --init_model_path <åˆå§‹åŒ–æ¨¡å‹è·¯å¾„>
"""

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from adapterlib.datautils import get_calib_data
import numpy as np
from tabulate import tabulate


@torch.no_grad()
def get_quick_stats(model, calib_loader, max_samples=50):
    """å¿«é€Ÿè·å–é›¶ç©ºé—´ç»Ÿè®¡"""
    model.eval()

    adapter_layers = {}
    for name, module in model.named_modules():
        if hasattr(module, 'ALinear') and hasattr(module, 'BLinear'):
            adapter_layers[name] = {
                'module': module,
                'cached_inputs': []
            }

    # æ”¶é›†æ¿€æ´»
    def make_hook(layer_name):
        def hook(module, input, output):
            if len(adapter_layers[layer_name]['cached_inputs']) < max_samples:
                inp = input[0].detach()
                if inp.dim() == 3:
                    inp = inp.reshape(-1, inp.size(-1))
                num_to_cache = min(max_samples - len(adapter_layers[layer_name]['cached_inputs']), inp.size(0))
                adapter_layers[layer_name]['cached_inputs'].append(inp[:num_to_cache].cpu())
        return hook

    handles = []
    for name, info in adapter_layers.items():
        handle = info['module'].register_forward_hook(make_hook(name))
        handles.append(handle)

    for batch in calib_loader:
        batch = {k: v.to(model.device) for k, v in batch.items()}
        model(**batch)
        if all(len(info['cached_inputs']) >= max_samples for info in adapter_layers.values()):
            break

    for handle in handles:
        handle.remove()

    # åˆå¹¶å¹¶è®¡ç®—ç»Ÿè®¡
    stats = []
    for name, info in adapter_layers.items():
        if len(info['cached_inputs']) == 0:
            continue

        X = torch.cat(info['cached_inputs'], dim=0).float()
        module = info['module']

        B_weight = module.BLinear.weight.data.cpu().float()
        A_weight = module.ALinear.weight.data.cpu().float()

        BX = B_weight @ X.t()
        ABX = A_weight @ BX

        X_norm = torch.norm(X, dim=1).mean().item()
        BX_norm = torch.norm(BX, dim=0).mean().item()
        ABX_norm = torch.norm(ABX, dim=0).mean().item()

        stats.append({
            'layer': name,
            'BX_ratio': BX_norm / X_norm if X_norm > 0 else 0,
            'ABX_ratio': ABX_norm / X_norm if X_norm > 0 else 0,
            'A_norm': torch.norm(A_weight).item(),
            'B_norm': torch.norm(B_weight).item()
        })

    return stats


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--trained_model_path", type=str, required=True)
    parser.add_argument("--init_model_path", type=str, required=True)
    parser.add_argument("--calib_dataset", type=str, default="wikitext2")
    parser.add_argument("--calib_loader_size", type=int, default=8)
    parser.add_argument("--max_samples", type=int, default=50)
    args = parser.parse_args()

    print("="*100)
    print("åˆå§‹åŒ– vs è®­ç»ƒåé›¶ç©ºé—´å¯¹æ¯”")
    print("="*100)

    # åŠ è½½åˆå§‹åŒ–æ¨¡å‹
    print("\n[1/4] åŠ è½½åˆå§‹åŒ–æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.init_model_path, trust_remote_code=True)
    init_model = AutoModelForCausalLM.from_pretrained(
        args.init_model_path,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    calib_loader = get_calib_data(
        args.calib_dataset, tokenizer, args.init_model_path,
        args.calib_loader_size, seed=42
    )

    print("[2/4] è®¡ç®—åˆå§‹åŒ–æ¨¡å‹ç»Ÿè®¡...")
    init_stats = get_quick_stats(init_model, calib_loader, args.max_samples)
    del init_model
    torch.cuda.empty_cache()

    # åŠ è½½è®­ç»ƒåæ¨¡å‹
    print("[3/4] åŠ è½½è®­ç»ƒåæ¨¡å‹...")
    trained_model = AutoModelForCausalLM.from_pretrained(
        args.trained_model_path,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    print("[4/4] è®¡ç®—è®­ç»ƒåæ¨¡å‹ç»Ÿè®¡...")
    trained_stats = get_quick_stats(trained_model, calib_loader, args.max_samples)

    # å¯¹æ¯”
    print("\n" + "="*100)
    print("å±‚çº§å¯¹æ¯”")
    print("="*100)

    table_data = []
    for init, trained in zip(init_stats, trained_stats):
        layer_name = init['layer'].split('.')[-1]  # ç®€åŒ–å±‚å
        table_data.append([
            layer_name,
            f"{init['BX_ratio']:.2e}",
            f"{trained['BX_ratio']:.2e}",
            f"{trained['BX_ratio'] / init['BX_ratio']:.2f}x" if init['BX_ratio'] > 0 else "N/A",
            f"{init['ABX_ratio']:.2e}",
            f"{trained['ABX_ratio']:.2e}",
            f"{trained['ABX_ratio'] / init['ABX_ratio']:.0f}x" if init['ABX_ratio'] > 1e-10 else "âˆ",
            f"{init['A_norm']:.2f}",
            f"{trained['A_norm']:.2f}",
        ])

    headers = [
        "Layer",
        "||BX||/||X|| (åˆå§‹)",
        "||BX||/||X|| (è®­ç»ƒå)",
        "å˜åŒ–å€æ•°",
        "||ABX||/||X|| (åˆå§‹)",
        "||ABX||/||X|| (è®­ç»ƒå)",
        "å˜åŒ–å€æ•°",
        "||A|| (åˆå§‹)",
        "||A|| (è®­ç»ƒå)",
    ]

    print(tabulate(table_data, headers=headers, tablefmt="grid"))

    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "="*100)
    print("æ±‡æ€»ç»Ÿè®¡")
    print("="*100)

    init_avg_BX = np.mean([s['BX_ratio'] for s in init_stats])
    trained_avg_BX = np.mean([s['BX_ratio'] for s in trained_stats])

    init_avg_ABX = np.mean([s['ABX_ratio'] for s in init_stats])
    trained_avg_ABX = np.mean([s['ABX_ratio'] for s in trained_stats])

    init_avg_A = np.mean([s['A_norm'] for s in init_stats])
    trained_avg_A = np.mean([s['A_norm'] for s in trained_stats])

    summary_data = [
        ["||BX|| / ||X|| (å¹³å‡)", f"{init_avg_BX:.2e}", f"{trained_avg_BX:.2e}",
         f"{trained_avg_BX / init_avg_BX:.2f}x" if init_avg_BX > 0 else "N/A"],
        ["||ABX|| / ||X|| (å¹³å‡)", f"{init_avg_ABX:.2e}", f"{trained_avg_ABX:.2e}",
         "âˆ" if init_avg_ABX < 1e-10 else f"{trained_avg_ABX / init_avg_ABX:.0f}x"],
        ["||A|| (å¹³å‡)", f"{init_avg_A:.2f}", f"{trained_avg_A:.2f}",
         "âˆ" if init_avg_A < 1e-10 else f"{trained_avg_A / init_avg_A:.0f}x"],
    ]

    print(tabulate(summary_data, headers=["æŒ‡æ ‡", "åˆå§‹åŒ–", "è®­ç»ƒå", "å˜åŒ–"],
                   tablefmt="grid"))

    # ç»“è®º
    print("\n" + "="*100)
    print("ğŸ“Š åˆ†æç»“è®º")
    print("="*100)

    # é›¶ç©ºé—´ä¿æŒæ€§
    BX_change = trained_avg_BX / init_avg_BX if init_avg_BX > 0 else float('inf')
    print(f"\n1ï¸âƒ£  é›¶ç©ºé—´ä¿æŒæ€§ (||BX|| / ||X||):")
    if BX_change < 1.5:
        print(f"   âœ… å˜åŒ– {BX_change:.2f}x - é›¶ç©ºé—´å±æ€§ä¿æŒè‰¯å¥½")
    elif BX_change < 3.0:
        print(f"   âš ï¸  å˜åŒ– {BX_change:.2f}x - é›¶ç©ºé—´æœ‰è½»å¾®é€€åŒ–")
    else:
        print(f"   âŒ å˜åŒ– {BX_change:.2f}x - é›¶ç©ºé—´æ˜¾è‘—é€€åŒ–ï¼Œæ£€æŸ¥ BLinear æ˜¯å¦è¢«å†»ç»“")

    # é€‚é…å™¨æ›´æ–°
    print(f"\n2ï¸âƒ£  é€‚é…å™¨æ›´æ–° (||ABX|| / ||X||):")
    if trained_avg_ABX > 1e-4:
        print(f"   âœ… è®­ç»ƒå {trained_avg_ABX:.2e} - é€‚é…å™¨å­¦åˆ°äº†æœ‰æ•ˆæ›´æ–°")
    elif trained_avg_ABX > 1e-6:
        print(f"   âš ï¸  è®­ç»ƒå {trained_avg_ABX:.2e} - é€‚é…å™¨æ›´æ–°è¾ƒå°")
    else:
        print(f"   âŒ è®­ç»ƒå {trained_avg_ABX:.2e} - é€‚é…å™¨å‡ ä¹æ²¡å˜åŒ–")

    # A æƒé‡å˜åŒ–
    print(f"\n3ï¸âƒ£  ALinear æƒé‡ (||A||):")
    if trained_avg_A > init_avg_A * 10:
        print(f"   âœ… ä» {init_avg_A:.2f} â†’ {trained_avg_A:.2f} - æƒé‡æ˜¾è‘—æ›´æ–°")
    elif trained_avg_A > init_avg_A * 2:
        print(f"   âš ï¸  ä» {init_avg_A:.2f} â†’ {trained_avg_A:.2f} - æƒé‡æœ‰æ›´æ–°ä½†è¾ƒå°")
    else:
        print(f"   âŒ ä» {init_avg_A:.2f} â†’ {trained_avg_A:.2f} - æƒé‡å‡ ä¹æ²¡å˜åŒ–")

    print("\n" + "="*100)


if __name__ == "__main__":
    main()
