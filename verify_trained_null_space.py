"""
è®­ç»ƒåé›¶ç©ºé—´éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯ LoRA-Null-v2 è®­ç»ƒåæ¨¡å‹çš„é›¶ç©ºé—´å±æ€§

è¿è¡Œæ–¹å¼:
python verify_trained_null_space.py --trained_model_path <è®­ç»ƒåæ¨¡å‹è·¯å¾„>
"""

import argparse
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer
from adapterlib.datautils import get_calib_data
from tqdm import tqdm
import numpy as np
import os


@torch.no_grad()
def collect_activations_and_verify(model, calib_loader, max_samples=100):
    """
    æ”¶é›†æ¿€æ´»å€¼å¹¶éªŒè¯é›¶ç©ºé—´å±æ€§

    Args:
        model: è®­ç»ƒåçš„æ¨¡å‹
        calib_loader: æ ¡å‡†æ•°æ®åŠ è½½å™¨
        max_samples: æœ€å¤šæ”¶é›†çš„æ ·æœ¬æ•°
    """
    model.eval()

    # æ‰¾åˆ°æ‰€æœ‰ CorDA_adapter å±‚
    adapter_layers = {}
    for name, module in model.named_modules():
        if hasattr(module, 'ALinear') and hasattr(module, 'BLinear'):
            adapter_layers[name] = {
                'module': module,
                'cached_inputs': [],
                'input_count': 0
            }

    print(f"\næ‰¾åˆ° {len(adapter_layers)} ä¸ª CorDA_adapter å±‚")

    # æ³¨å†Œ hook æ”¶é›†è¾“å…¥
    def make_hook(layer_name):
        def hook(module, input, output):
            if adapter_layers[layer_name]['input_count'] < max_samples:
                inp = input[0].detach()
                if inp.dim() == 3:  # (batch, seq_len, dim)
                    inp = inp.reshape(-1, inp.size(-1))  # (batch*seq_len, dim)

                # åªå–å‰å‡ ä¸ªæ ·æœ¬
                num_to_cache = min(max_samples - adapter_layers[layer_name]['input_count'], inp.size(0))
                adapter_layers[layer_name]['cached_inputs'].append(inp[:num_to_cache].cpu())
                adapter_layers[layer_name]['input_count'] += num_to_cache
        return hook

    # æ³¨å†Œæ‰€æœ‰ hooks
    handles = []
    for name, info in adapter_layers.items():
        handle = info['module'].register_forward_hook(make_hook(name))
        handles.append(handle)

    # å‰å‘ä¼ æ’­æ”¶é›†æ•°æ®
    print("\næ”¶é›†æ¿€æ´»å€¼...")
    for batch in tqdm(calib_loader):
        batch = {k: v.to(model.device) for k, v in batch.items()}
        model(**batch)

        # æ£€æŸ¥æ˜¯å¦æ”¶é›†å¤Ÿäº†
        if all(info['input_count'] >= max_samples for info in adapter_layers.values()):
            break

    # ç§»é™¤ hooks
    for handle in handles:
        handle.remove()

    # åˆå¹¶ç¼“å­˜çš„è¾“å…¥
    for name, info in adapter_layers.items():
        if len(info['cached_inputs']) > 0:
            info['cached_inputs'] = torch.cat(info['cached_inputs'], dim=0)

    return adapter_layers


@torch.no_grad()
def verify_null_space_properties(adapter_layers, verbose=True):
    """
    éªŒè¯é›¶ç©ºé—´å±æ€§

    Args:
        adapter_layers: åŒ…å«é€‚é…å™¨å±‚å’Œç¼“å­˜è¾“å…¥çš„å­—å…¸
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
    """
    print("\n" + "="*80)
    print("è®­ç»ƒåé›¶ç©ºé—´éªŒè¯")
    print("="*80)

    all_stats = []

    for name, info in adapter_layers.items():
        module = info['module']
        X = info['cached_inputs']

        if X is None or len(X) == 0:
            continue

        X = X.float()  # (num_samples, in_features)

        # è·å– B çš„æƒé‡ (BLinear: in_features -> rank)
        B_weight = module.BLinear.weight.data.cpu().float()  # (rank, in_features)

        # è·å– A çš„æƒé‡ (ALinear: rank -> out_features)
        A_weight = module.ALinear.weight.data.cpu().float()  # (out_features, rank)

        # è®¡ç®— BX
        BX = B_weight @ X.t()  # (rank, num_samples)

        # è®¡ç®— ABX
        ABX = A_weight @ BX  # (out_features, num_samples)

        # è®¡ç®—æ®‹å·®é¡¹ W_residual @ X
        W_residual = module.weight_residual.data.cpu().float()  # (out_features, in_features)
        ResidualX = W_residual @ X.t()  # (out_features, num_samples)

        # å®Œæ•´è¾“å‡º
        FullOutput = ABX + ResidualX  # (out_features, num_samples)

        # è®¡ç®—èŒƒæ•°ç»Ÿè®¡
        X_norm = torch.norm(X, dim=1).mean().item()
        BX_norm = torch.norm(BX, dim=0).mean().item()
        ABX_norm = torch.norm(ABX, dim=0).mean().item()
        ResidualX_norm = torch.norm(ResidualX, dim=0).mean().item()
        FullOutput_norm = torch.norm(FullOutput, dim=0).mean().item()

        # è®¡ç®—æƒé‡ç»Ÿè®¡
        B_norm = torch.norm(B_weight).item()
        A_norm = torch.norm(A_weight).item()
        Residual_norm = torch.norm(W_residual).item()

        stats = {
            'layer_name': name,
            'num_samples': X.size(0),
            'X_norm': X_norm,
            'BX_norm': BX_norm,
            'ABX_norm': ABX_norm,
            'ResidualX_norm': ResidualX_norm,
            'FullOutput_norm': FullOutput_norm,
            'BX_ratio': BX_norm / X_norm if X_norm > 0 else 0,
            'ABX_ratio': ABX_norm / X_norm if X_norm > 0 else 0,
            'adapter_contribution': ABX_norm / FullOutput_norm if FullOutput_norm > 0 else 0,
            'B_weight_norm': B_norm,
            'A_weight_norm': A_norm,
            'Residual_weight_norm': Residual_norm,
        }

        all_stats.append(stats)

        if verbose:
            print(f"\n{'='*80}")
            print(f"Layer: {name}")
            print(f"{'='*80}")
            print(f"è¾“å…¥æ ·æœ¬æ•°: {stats['num_samples']}")
            print(f"è¾“å…¥ç»´åº¦: {X.shape[1]}, ç§©: {B_weight.shape[0]}, è¾“å‡ºç»´åº¦: {A_weight.shape[0]}")
            print(f"-" * 80)
            print(f"æ¿€æ´»å€¼èŒƒæ•°:")
            print(f"  ||X|| (avg):           {stats['X_norm']:.6f}")
            print(f"  ||BX|| (avg):          {stats['BX_norm']:.6f}")
            print(f"  ||ABX|| (avg):         {stats['ABX_norm']:.6f}")
            print(f"  ||W_residual @ X||:    {stats['ResidualX_norm']:.6f}")
            print(f"  ||å®Œæ•´è¾“å‡º||:           {stats['FullOutput_norm']:.6f}")
            print(f"-" * 80)
            print(f"é›¶ç©ºé—´æŒ‡æ ‡:")
            print(f"  ||BX|| / ||X||:        {stats['BX_ratio']:.6e} {'âœ“ ä»ç„¶å¾ˆå°' if stats['BX_ratio'] < 0.1 else 'âš  å˜å¤§äº†'}")
            print(f"  ||ABX|| / ||X||:       {stats['ABX_ratio']:.6e} {'âœ“ é€‚é…å™¨æ›´æ–°æœ‰æ•ˆ' if stats['ABX_ratio'] > 1e-6 else 'âš  é€‚é…å™¨å‡ ä¹æ²¡å˜åŒ–'}")
            print(f"-" * 80)
            print(f"é€‚é…å™¨è´¡çŒ®:")
            print(f"  ||ABX|| / ||è¾“å‡º||:    {stats['adapter_contribution']:.6e} (é€‚é…å™¨åœ¨æ€»è¾“å‡ºä¸­çš„ç›¸å¯¹è´¡çŒ®)")
            print(f"-" * 80)
            print(f"æƒé‡èŒƒæ•°:")
            print(f"  ||B||_F:               {stats['B_weight_norm']:.6f}")
            print(f"  ||A||_F:               {stats['A_weight_norm']:.6f}")
            print(f"  ||W_residual||_F:      {stats['Residual_weight_norm']:.6f}")
            print(f"{'='*80}")

    # æ±‡æ€»ç»Ÿè®¡
    if len(all_stats) > 0:
        print(f"\n{'='*80}")
        print("æ±‡æ€»ç»Ÿè®¡ï¼ˆæ‰€æœ‰å±‚å¹³å‡ï¼‰")
        print(f"{'='*80}")
        avg_BX_ratio = np.mean([s['BX_ratio'] for s in all_stats])
        avg_ABX_ratio = np.mean([s['ABX_ratio'] for s in all_stats])
        avg_adapter_contrib = np.mean([s['adapter_contribution'] for s in all_stats])

        print(f"å¹³å‡ ||BX|| / ||X||:        {avg_BX_ratio:.6e}")
        print(f"å¹³å‡ ||ABX|| / ||X||:       {avg_ABX_ratio:.6e}")
        print(f"å¹³å‡é€‚é…å™¨è´¡çŒ®æ¯”:            {avg_adapter_contrib:.6e}")
        print(f"{'='*80}")

        # è§£é‡Š
        print("\nğŸ“Š ç»“æœè§£è¯»:")
        print("-" * 80)
        if avg_BX_ratio < 0.1:
            print("âœ… ||BX|| / ||X|| < 0.1: é›¶ç©ºé—´å±æ€§ä¿æŒè‰¯å¥½")
            print("   è¾“å…¥æŠ•å½±åˆ° B ç©ºé—´åå¹…åº¦ä»ç„¶å¾ˆå°ï¼Œè¯´æ˜ B ä»åœ¨è¿‘ä¼¼é›¶ç©ºé—´ä¸­")
        else:
            print("âš ï¸  ||BX|| / ||X|| >= 0.1: é›¶ç©ºé—´å±æ€§æœ‰æ‰€é€€åŒ–")
            print("   è¿™å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒä¸­ BLinear æƒé‡æœ‰æ›´æ–°ï¼ˆè™½ç„¶ç†è®ºä¸Šåº”è¯¥å†»ç»“ï¼‰")

        print()
        if avg_ABX_ratio > 1e-4:
            print(f"âœ… ||ABX|| / ||X|| = {avg_ABX_ratio:.2e}: é€‚é…å™¨å­¦åˆ°äº†æœ‰æ•ˆçš„æ›´æ–°")
            print("   ALinear æƒé‡å·²ç»ä»åˆå§‹çš„0æ›´æ–°åˆ°æœ‰æ„ä¹‰çš„å€¼")
        elif avg_ABX_ratio > 1e-6:
            print(f"âš ï¸  ||ABX|| / ||X|| = {avg_ABX_ratio:.2e}: é€‚é…å™¨æ›´æ–°è¾ƒå°")
            print("   å¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæˆ–æ›´å¤§çš„å­¦ä¹ ç‡")
        else:
            print(f"âŒ ||ABX|| / ||X|| = {avg_ABX_ratio:.2e}: é€‚é…å™¨å‡ ä¹æ²¡æœ‰å­¦åˆ°ä¸œè¥¿")
            print("   æ£€æŸ¥è®­ç»ƒæ˜¯å¦æ­£å¸¸è¿›è¡Œï¼ŒALinear æ˜¯å¦è¢«å†»ç»“äº†")

        print()
        if avg_adapter_contrib > 0.01:
            print(f"âœ… é€‚é…å™¨è´¡çŒ® {avg_adapter_contrib:.2%}: å¯¹æ¨¡å‹è¾“å‡ºæœ‰æ˜æ˜¾å½±å“")
        else:
            print(f"âš ï¸  é€‚é…å™¨è´¡çŒ® {avg_adapter_contrib:.2%}: å¯¹æ¨¡å‹è¾“å‡ºå½±å“è¾ƒå°")

        print("-" * 80)

    return all_stats


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯è®­ç»ƒå LoRA-Null-v2 æ¨¡å‹çš„é›¶ç©ºé—´å±æ€§")
    parser.add_argument("--trained_model_path", type=str, required=True,
                        help="è®­ç»ƒåæ¨¡å‹è·¯å¾„ï¼ˆé€šå¸¸æ˜¯ output_dir/ftï¼‰")
    parser.add_argument("--calib_dataset", type=str, default="wikitext2",
                        choices=["wikitext2", "c4", "ptb", "triviaqa", "nqopen"],
                        help="ç”¨äºæ”¶é›†æ¿€æ´»çš„æ•°æ®é›†")
    parser.add_argument("--calib_loader_size", type=int, default=16,
                        help="æ ¡å‡†æ ·æœ¬æ•°é‡")
    parser.add_argument("--max_activation_samples", type=int, default=100,
                        help="æ¯å±‚æœ€å¤šæ”¶é›†çš„æ¿€æ´»æ ·æœ¬æ•°")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save_stats", type=str, default=None,
                        help="ä¿å­˜ç»Ÿè®¡ç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼ˆ.jsonï¼‰")

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)

    print("="*80)
    print("LoRA-Null-v2 è®­ç»ƒåé›¶ç©ºé—´éªŒè¯")
    print("="*80)
    print(f"è®­ç»ƒåæ¨¡å‹è·¯å¾„: {args.trained_model_path}")
    print(f"æ ¡å‡†æ•°æ®é›†: {args.calib_dataset}")
    print(f"æ ¡å‡†æ ·æœ¬æ•°: {args.calib_loader_size}")
    print("="*80)

    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(args.trained_model_path):
        raise ValueError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.trained_model_path}")

    # åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
    print("\n[1/3] åŠ è½½è®­ç»ƒåçš„æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.trained_model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.trained_model_path,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    print(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")

    # æ”¶é›†æ ¡å‡†æ•°æ®
    print("\n[2/3] æ”¶é›†æ ¡å‡†æ•°æ®...")
    calib_loader = get_calib_data(
        args.calib_dataset,
        tokenizer,
        args.trained_model_path,
        args.calib_loader_size,
        seed=args.seed
    )

    # æ”¶é›†æ¿€æ´»å€¼
    print("\n[3/3] æ”¶é›†æ¿€æ´»å€¼å¹¶éªŒè¯é›¶ç©ºé—´å±æ€§...")
    adapter_layers = collect_activations_and_verify(
        model,
        calib_loader,
        max_samples=args.max_activation_samples
    )

    # éªŒè¯é›¶ç©ºé—´å±æ€§
    stats = verify_null_space_properties(adapter_layers, verbose=True)

    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    if args.save_stats and len(stats) > 0:
        import json
        output = {
            'model_path': args.trained_model_path,
            'calib_dataset': args.calib_dataset,
            'num_layers': len(stats),
            'stats': stats
        }
        with open(args.save_stats, 'w') as f:
            json.dump(output, f, indent=2)
        print(f"\nç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {args.save_stats}")

    print("\nâœ… éªŒè¯å®Œæˆ!")


if __name__ == "__main__":
    main()
